{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmCZBny5qSHB"
      },
      "source": [
        "## Optuna para Aves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGKiGl8_-tX6",
        "outputId": "ec81ae3f-f7ca-4fde-fd1d-e1678f63f0d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-07-25 17:09:47,290] A new study created in RDB with name: XGBoost_tuning\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Afinando: XGBoost ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:09:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:09:48,800] Trial 0 finished with value: 0.916030534351145 and parameters: {'n_estimators': 50, 'max_depth': 30, 'learning_rate': 0.27992378565962994, 'subsample': 0.9554315594749954, 'colsample_bytree': 0.6279566494051945, 'gamma': 1.2049640092348017, 'min_child_weight': 2, 'reg_alpha': 0.005830752829298216, 'reg_lambda': 2.60954952644566e-05}. Best is trial 0 with value: 0.916030534351145.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:09:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:09:55,593] Trial 1 finished with value: 0.918648310387985 and parameters: {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.043608198256540304, 'subsample': 0.9700164367561661, 'colsample_bytree': 0.670933652819302, 'gamma': 0.4206442512986919, 'min_child_weight': 1, 'reg_alpha': 3.9707094441626026e-05, 'reg_lambda': 0.4500826982692626}. Best is trial 1 with value: 0.918648310387985.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:09:55] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:09:57,031] Trial 2 finished with value: 0.9305912596401028 and parameters: {'n_estimators': 250, 'max_depth': 5, 'learning_rate': 0.10752623936035248, 'subsample': 0.833982197522736, 'colsample_bytree': 0.8329211491757728, 'gamma': 0.5368201185347538, 'min_child_weight': 6, 'reg_alpha': 3.979192231505574e-05, 'reg_lambda': 2.7000617914739144e-07}. Best is trial 2 with value: 0.9305912596401028.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:09:57] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:02,167] Trial 3 finished with value: 0.8685446009389671 and parameters: {'n_estimators': 300, 'max_depth': 15, 'learning_rate': 0.011602668615569895, 'subsample': 0.7649665873828727, 'colsample_bytree': 0.9160648522216064, 'gamma': 4.985885420176642, 'min_child_weight': 10, 'reg_alpha': 2.6641763925276793e-06, 'reg_lambda': 0.001847117508578474}. Best is trial 2 with value: 0.9305912596401028.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:02] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:03,743] Trial 4 finished with value: 0.9281210592686002 and parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.07076200664515392, 'subsample': 0.9728359961963959, 'colsample_bytree': 0.5082123231565487, 'gamma': 2.8263248879533993, 'min_child_weight': 5, 'reg_alpha': 0.009053258762904916, 'reg_lambda': 1.3174564671942848e-08}. Best is trial 2 with value: 0.9305912596401028.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:03] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:10,134] Trial 5 finished with value: 0.8558246828143022 and parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.010097349943876736, 'subsample': 0.5012070600638754, 'colsample_bytree': 0.9883659396714461, 'gamma': 1.042979302809623, 'min_child_weight': 3, 'reg_alpha': 1.0314575742771007e-08, 'reg_lambda': 0.00015726997032593663}. Best is trial 2 with value: 0.9305912596401028.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:21,263] Trial 6 finished with value: 0.9118012422360249 and parameters: {'n_estimators': 300, 'max_depth': 35, 'learning_rate': 0.012314810479025174, 'subsample': 0.9914302810453268, 'colsample_bytree': 0.7372516864919858, 'gamma': 0.3918518109861846, 'min_child_weight': 4, 'reg_alpha': 0.0011578784729144407, 'reg_lambda': 9.270193884465295e-08}. Best is trial 2 with value: 0.9305912596401028.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:21] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:22,967] Trial 7 finished with value: 0.8859857482185273 and parameters: {'n_estimators': 100, 'max_depth': 30, 'learning_rate': 0.03745640583492719, 'subsample': 0.6363184000284259, 'colsample_bytree': 0.7147959612993224, 'gamma': 1.6469789164452102, 'min_child_weight': 10, 'reg_alpha': 1.5416758776860216e-08, 'reg_lambda': 0.21685991245967293}. Best is trial 2 with value: 0.9305912596401028.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:23] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:24,800] Trial 8 finished with value: 0.7845993756503642 and parameters: {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.018261358151640848, 'subsample': 0.5254628542866197, 'colsample_bytree': 0.6667749602183323, 'gamma': 0.9708639474246533, 'min_child_weight': 8, 'reg_alpha': 0.257498107582037, 'reg_lambda': 4.5907473155685847e-07}. Best is trial 2 with value: 0.9305912596401028.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:25] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:26,506] Trial 9 finished with value: 0.924812030075188 and parameters: {'n_estimators': 300, 'max_depth': 20, 'learning_rate': 0.0813317549017024, 'subsample': 0.6323228008149351, 'colsample_bytree': 0.810310378545331, 'gamma': 3.6755150906898693, 'min_child_weight': 6, 'reg_alpha': 0.09196721927687183, 'reg_lambda': 1.0290246195042934e-06}. Best is trial 2 with value: 0.9305912596401028.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:26] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:27,826] Trial 10 finished with value: 0.9326556543837357 and parameters: {'n_estimators': 250, 'max_depth': 50, 'learning_rate': 0.17924967980399673, 'subsample': 0.8336323421333139, 'colsample_bytree': 0.8450045515001706, 'gamma': 2.1719509646920523, 'min_child_weight': 7, 'reg_alpha': 2.1225815883806807e-06, 'reg_lambda': 1.1574507094738504e-05}. Best is trial 10 with value: 0.9326556543837357.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:28] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:30,265] Trial 11 finished with value: 0.9284818067754078 and parameters: {'n_estimators': 250, 'max_depth': 50, 'learning_rate': 0.16848613517592861, 'subsample': 0.8265181957486288, 'colsample_bytree': 0.8419013794224792, 'gamma': 2.93173025534036, 'min_child_weight': 7, 'reg_alpha': 2.693028559613177e-06, 'reg_lambda': 9.827486765970094e-06}. Best is trial 10 with value: 0.9326556543837357.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:30] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:31,776] Trial 12 finished with value: 0.9287531806615776 and parameters: {'n_estimators': 250, 'max_depth': 50, 'learning_rate': 0.13811805862954424, 'subsample': 0.8400577336772818, 'colsample_bytree': 0.8543759954633157, 'gamma': 1.91029143654107, 'min_child_weight': 8, 'reg_alpha': 1.4068194508080862e-06, 'reg_lambda': 0.0019371549897721169}. Best is trial 10 with value: 0.9326556543837357.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:32] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:33,095] Trial 13 finished with value: 0.9211514392991239 and parameters: {'n_estimators': 250, 'max_depth': 40, 'learning_rate': 0.14097304613312064, 'subsample': 0.8647393843731951, 'colsample_bytree': 0.9095435308229016, 'gamma': 3.704415056079956, 'min_child_weight': 6, 'reg_alpha': 6.004192418352772e-05, 'reg_lambda': 2.2621171162629594e-06}. Best is trial 10 with value: 0.9326556543837357.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:33] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:34,052] Trial 14 finished with value: 0.9205548549810845 and parameters: {'n_estimators': 200, 'max_depth': 40, 'learning_rate': 0.28178787891878404, 'subsample': 0.7333240337531819, 'colsample_bytree': 0.7890628978445144, 'gamma': 2.214497559671147, 'min_child_weight': 7, 'reg_alpha': 2.8881915380643954e-07, 'reg_lambda': 0.00017427831703341622}. Best is trial 10 with value: 0.9326556543837357.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:34] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:37,024] Trial 15 finished with value: 0.942381562099872 and parameters: {'n_estimators': 150, 'max_depth': 20, 'learning_rate': 0.09320594541435827, 'subsample': 0.9037971740382322, 'colsample_bytree': 0.9948442904976956, 'gamma': 0.056005778158759845, 'min_child_weight': 4, 'reg_alpha': 0.00029609198923313953, 'reg_lambda': 6.220702837487903e-08}. Best is trial 15 with value: 0.942381562099872.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:37] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:44,279] Trial 16 finished with value: 0.906832298136646 and parameters: {'n_estimators': 150, 'max_depth': 20, 'learning_rate': 0.02810901309892209, 'subsample': 0.9049328481997263, 'colsample_bytree': 0.988819519518265, 'gamma': 0.07712762164569192, 'min_child_weight': 4, 'reg_alpha': 0.0006326992829479306, 'reg_lambda': 1.309634668626372e-08}. Best is trial 15 with value: 0.942381562099872.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:44] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:45,333] Trial 17 finished with value: 0.9081885856079405 and parameters: {'n_estimators': 150, 'max_depth': 25, 'learning_rate': 0.17806059948423272, 'subsample': 0.7468105321306273, 'colsample_bytree': 0.920364493821741, 'gamma': 4.751321690048114, 'min_child_weight': 4, 'reg_alpha': 2.944214750496273e-07, 'reg_lambda': 0.0070364650540235656}. Best is trial 15 with value: 0.942381562099872.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:45] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:47,220] Trial 18 finished with value: 0.9232704402515723 and parameters: {'n_estimators': 150, 'max_depth': 45, 'learning_rate': 0.0699010916443094, 'subsample': 0.9057579535116642, 'colsample_bytree': 0.9477892358668586, 'gamma': 3.57842826223293, 'min_child_weight': 9, 'reg_alpha': 0.00034908683094773995, 'reg_lambda': 7.942645078418844e-06}. Best is trial 15 with value: 0.942381562099872.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:49,223] Trial 19 finished with value: 0.9390862944162437 and parameters: {'n_estimators': 200, 'max_depth': 25, 'learning_rate': 0.10372852966594616, 'subsample': 0.7943862132353365, 'colsample_bytree': 0.8702266013785176, 'gamma': 1.5258288550521768, 'min_child_weight': 5, 'reg_alpha': 1.21135297731045e-05, 'reg_lambda': 7.441805053956832e-08}. Best is trial 15 with value: 0.942381562099872.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:51,776] Trial 20 finished with value: 0.9028290282902829 and parameters: {'n_estimators': 50, 'max_depth': 25, 'learning_rate': 0.05491381055355031, 'subsample': 0.6693597679025978, 'colsample_bytree': 0.8801379604071045, 'gamma': 1.5388775450515393, 'min_child_weight': 2, 'reg_alpha': 1.4948937960269416e-05, 'reg_lambda': 8.209777533526478e-08}. Best is trial 15 with value: 0.942381562099872.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:52] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:54,026] Trial 21 finished with value: 0.9236641221374046 and parameters: {'n_estimators': 200, 'max_depth': 20, 'learning_rate': 0.10532293728555957, 'subsample': 0.8930010795643294, 'colsample_bytree': 0.7867017513373692, 'gamma': 2.3124814904648874, 'min_child_weight': 5, 'reg_alpha': 1.15947772838364e-05, 'reg_lambda': 4.870693011311176e-08}. Best is trial 15 with value: 0.942381562099872.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:54] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:55,172] Trial 22 finished with value: 0.9304677623261695 and parameters: {'n_estimators': 150, 'max_depth': 35, 'learning_rate': 0.2163049788559649, 'subsample': 0.7978794730875793, 'colsample_bytree': 0.9438302689497493, 'gamma': 2.7517334100916706, 'min_child_weight': 7, 'reg_alpha': 1.3449644625492115e-07, 'reg_lambda': 2.8638237443862696e-06}. Best is trial 15 with value: 0.942381562099872.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:55] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:57,242] Trial 23 finished with value: 0.9285714285714286 and parameters: {'n_estimators': 200, 'max_depth': 25, 'learning_rate': 0.10057093818233251, 'subsample': 0.6948557494666916, 'colsample_bytree': 0.872764836637904, 'gamma': 1.8230890575739473, 'min_child_weight': 3, 'reg_alpha': 0.00021478279684686232, 'reg_lambda': 2.5909369584220405e-07}. Best is trial 15 with value: 0.942381562099872.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:57] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:10:59,215] Trial 24 finished with value: 0.9383033419023136 and parameters: {'n_estimators': 250, 'max_depth': 15, 'learning_rate': 0.1278322977138129, 'subsample': 0.7883890727466736, 'colsample_bytree': 0.7650670583167415, 'gamma': 0.7376739812325377, 'min_child_weight': 5, 'reg_alpha': 0.004426461373064791, 'reg_lambda': 2.9477978252964927e-05}. Best is trial 15 with value: 0.942381562099872.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:10:59] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:02,060] Trial 25 finished with value: 0.9326556543837357 and parameters: {'n_estimators': 150, 'max_depth': 15, 'learning_rate': 0.05833487339776534, 'subsample': 0.785612446281968, 'colsample_bytree': 0.7624126447727618, 'gamma': 0.03699510532662105, 'min_child_weight': 5, 'reg_alpha': 0.004308733151891946, 'reg_lambda': 0.06264236502513848}. Best is trial 15 with value: 0.942381562099872.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:02] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:05,654] Trial 26 finished with value: 0.9455958549222798 and parameters: {'n_estimators': 200, 'max_depth': 15, 'learning_rate': 0.09004078532872037, 'subsample': 0.7138380787002694, 'colsample_bytree': 0.56597071632727, 'gamma': 0.7530485910344991, 'min_child_weight': 3, 'reg_alpha': 0.041801195140935256, 'reg_lambda': 4.6732605578824736e-05}. Best is trial 26 with value: 0.9455958549222798.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:05] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:07,436] Trial 27 finished with value: 0.9336734693877551 and parameters: {'n_estimators': 100, 'max_depth': 20, 'learning_rate': 0.07824335783129341, 'subsample': 0.5842116667896149, 'colsample_bytree': 0.5133948027073824, 'gamma': 1.2911164397344914, 'min_child_weight': 3, 'reg_alpha': 0.01852340656561023, 'reg_lambda': 2.5292728105899185e-08}. Best is trial 26 with value: 0.9455958549222798.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:07] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:08,722] Trial 28 finished with value: 0.8661233993015134 and parameters: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.038347647456189494, 'subsample': 0.7293238228310078, 'colsample_bytree': 0.6021859851266008, 'gamma': 0.7162915544731201, 'min_child_weight': 1, 'reg_alpha': 0.6314090304527498, 'reg_lambda': 0.00048210251902407544}. Best is trial 26 with value: 0.9455958549222798.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:10,524] Trial 29 finished with value: 0.9271356783919598 and parameters: {'n_estimators': 50, 'max_depth': 30, 'learning_rate': 0.09228169667832645, 'subsample': 0.9373205900025028, 'colsample_bytree': 0.5668205505023097, 'gamma': 1.2229723220563566, 'min_child_weight': 2, 'reg_alpha': 0.0527869394794695, 'reg_lambda': 0.03369724918917846}. Best is trial 26 with value: 0.9455958549222798.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:11,784] Trial 30 finished with value: 0.9280205655526992 and parameters: {'n_estimators': 150, 'max_depth': 15, 'learning_rate': 0.2211758864436318, 'subsample': 0.7018744523967664, 'colsample_bytree': 0.6742877472845048, 'gamma': 1.4848991312699185, 'min_child_weight': 2, 'reg_alpha': 0.0016463089848577567, 'reg_lambda': 5.3354678493365324e-05}. Best is trial 26 with value: 0.9455958549222798.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:13,745] Trial 31 finished with value: 0.9446589446589446 and parameters: {'n_estimators': 250, 'max_depth': 15, 'learning_rate': 0.12735351592128116, 'subsample': 0.8051493319989885, 'colsample_bytree': 0.7184957271446221, 'gamma': 0.785314672358223, 'min_child_weight': 4, 'reg_alpha': 0.004314155347645372, 'reg_lambda': 3.322892104492431e-05}. Best is trial 26 with value: 0.9455958549222798.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:14] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:17,236] Trial 32 finished with value: 0.9455958549222798 and parameters: {'n_estimators': 200, 'max_depth': 20, 'learning_rate': 0.11759310406916311, 'subsample': 0.8699875973746698, 'colsample_bytree': 0.6138938384601136, 'gamma': 0.3226650347484365, 'min_child_weight': 4, 'reg_alpha': 0.034182444434294336, 'reg_lambda': 1.9352493717883223e-06}. Best is trial 26 with value: 0.9455958549222798.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:17] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:19,121] Trial 33 finished with value: 0.9503916449086162 and parameters: {'n_estimators': 200, 'max_depth': 20, 'learning_rate': 0.14377276965646646, 'subsample': 0.9370059089278516, 'colsample_bytree': 0.5685223856151826, 'gamma': 0.31313744469207505, 'min_child_weight': 4, 'reg_alpha': 0.06650242380747742, 'reg_lambda': 3.4458718946054895e-06}. Best is trial 33 with value: 0.9503916449086162.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:19] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:21,153] Trial 34 finished with value: 0.9458762886597938 and parameters: {'n_estimators': 250, 'max_depth': 15, 'learning_rate': 0.12565045208885675, 'subsample': 0.8616316882967399, 'colsample_bytree': 0.5721827839958256, 'gamma': 0.4268283944350796, 'min_child_weight': 3, 'reg_alpha': 0.025951657026846424, 'reg_lambda': 2.4842265781019763e-06}. Best is trial 33 with value: 0.9503916449086162.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:21] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:22,361] Trial 35 finished with value: 0.9506493506493506 and parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.22418415470889766, 'subsample': 0.9439161242228953, 'colsample_bytree': 0.5447353055951891, 'gamma': 0.3959332118976011, 'min_child_weight': 3, 'reg_alpha': 0.045381237991779275, 'reg_lambda': 3.0072833204664107e-06}. Best is trial 35 with value: 0.9506493506493506.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:23,379] Trial 36 finished with value: 0.9523809523809523 and parameters: {'n_estimators': 250, 'max_depth': 5, 'learning_rate': 0.23609343440151934, 'subsample': 0.9478447020285832, 'colsample_bytree': 0.5602621690070864, 'gamma': 0.5290264150139972, 'min_child_weight': 3, 'reg_alpha': 0.18986410547283059, 'reg_lambda': 5.715434211084247e-07}. Best is trial 36 with value: 0.9523809523809523.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:23] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:24,495] Trial 37 finished with value: 0.9460154241645244 and parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.23332886522928215, 'subsample': 0.9468220318369087, 'colsample_bytree': 0.5443308836453312, 'gamma': 0.38917692625130934, 'min_child_weight': 1, 'reg_alpha': 0.3047499126251573, 'reg_lambda': 7.212461128871351e-07}. Best is trial 36 with value: 0.9523809523809523.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:24] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:25,521] Trial 38 finished with value: 0.9498069498069498 and parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.2976609911940605, 'subsample': 0.9575651057170812, 'colsample_bytree': 0.5334641595830945, 'gamma': 0.31907203039876825, 'min_child_weight': 1, 'reg_alpha': 0.8500083315684275, 'reg_lambda': 4.500935009130007e-07}. Best is trial 36 with value: 0.9523809523809523.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:25] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:27,700] Trial 39 finished with value: 0.9345314505776636 and parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.29912240299826814, 'subsample': 0.9880143289142034, 'colsample_bytree': 0.646055274433414, 'gamma': 1.0061218805069536, 'min_child_weight': 1, 'reg_alpha': 0.13353744954360364, 'reg_lambda': 2.678049429779746e-07}. Best is trial 36 with value: 0.9523809523809523.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:27] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:28,964] Trial 40 finished with value: 0.941025641025641 and parameters: {'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.25319460208291616, 'subsample': 0.9335061576885263, 'colsample_bytree': 0.501221843599756, 'gamma': 0.23717455420776237, 'min_child_weight': 2, 'reg_alpha': 0.9322988061414936, 'reg_lambda': 4.869408181401893e-06}. Best is trial 36 with value: 0.9523809523809523.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:29] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:30,023] Trial 41 finished with value: 0.944516129032258 and parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.22835419155314116, 'subsample': 0.9553370269451761, 'colsample_bytree': 0.5934759727700403, 'gamma': 0.5660295747632326, 'min_child_weight': 1, 'reg_alpha': 0.30003464931730306, 'reg_lambda': 8.74319829461629e-07}. Best is trial 36 with value: 0.9523809523809523.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:30] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:31,476] Trial 42 finished with value: 0.9399477806788512 and parameters: {'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.19016865292718801, 'subsample': 0.9530106807300236, 'colsample_bytree': 0.5387565258003554, 'gamma': 0.49626838118261174, 'min_child_weight': 1, 'reg_alpha': 0.3220424427843369, 'reg_lambda': 6.402000119495852e-07}. Best is trial 36 with value: 0.9523809523809523.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:31] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:32,613] Trial 43 finished with value: 0.9393548387096774 and parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.25204046528692786, 'subsample': 0.9969619025786427, 'colsample_bytree': 0.5267910587456912, 'gamma': 0.2554234792721421, 'min_child_weight': 2, 'reg_alpha': 0.12774989843459314, 'reg_lambda': 1.6829252705838602e-07}. Best is trial 36 with value: 0.9523809523809523.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:32] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:34,015] Trial 44 finished with value: 0.9345314505776636 and parameters: {'n_estimators': 250, 'max_depth': 10, 'learning_rate': 0.15284347135682677, 'subsample': 0.9349993313878326, 'colsample_bytree': 0.5417979977906507, 'gamma': 1.0052969861488965, 'min_child_weight': 3, 'reg_alpha': 0.012876905205646575, 'reg_lambda': 1.0553346682329816e-06}. Best is trial 36 with value: 0.9523809523809523.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:34] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:35,249] Trial 45 finished with value: 0.9432989690721649 and parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.19981775036511526, 'subsample': 0.9667512327632064, 'colsample_bytree': 0.6315950411628045, 'gamma': 0.49496736861886903, 'min_child_weight': 1, 'reg_alpha': 0.41429158514463615, 'reg_lambda': 1.2199577979159075e-05}. Best is trial 36 with value: 0.9523809523809523.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:35] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:36,212] Trial 46 finished with value: 0.935031847133758 and parameters: {'n_estimators': 250, 'max_depth': 5, 'learning_rate': 0.2512289319216465, 'subsample': 0.924248333484728, 'colsample_bytree': 0.5789175099762627, 'gamma': 0.8893407656857598, 'min_child_weight': 2, 'reg_alpha': 0.09468094862220601, 'reg_lambda': 2.0729559718334047e-07}. Best is trial 36 with value: 0.9523809523809523.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:36] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:37,552] Trial 47 finished with value: 0.9260204081632653 and parameters: {'n_estimators': 250, 'max_depth': 10, 'learning_rate': 0.16054250621789828, 'subsample': 0.9819502851134538, 'colsample_bytree': 0.547711515986509, 'gamma': 1.2184671631284913, 'min_child_weight': 3, 'reg_alpha': 0.17500793433090356, 'reg_lambda': 4.888286296768499e-07}. Best is trial 36 with value: 0.9523809523809523.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:37] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:39,620] Trial 48 finished with value: 0.9393548387096774 and parameters: {'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.2886526932621538, 'subsample': 0.8780668033360784, 'colsample_bytree': 0.5229875563657963, 'gamma': 0.21355781998039997, 'min_child_weight': 2, 'reg_alpha': 0.762488176356919, 'reg_lambda': 6.50520500383897e-06}. Best is trial 36 with value: 0.9523809523809523.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:11:39] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 17:11:40,781] Trial 49 finished with value: 0.9446589446589446 and parameters: {'n_estimators': 250, 'max_depth': 5, 'learning_rate': 0.20098589321817945, 'subsample': 0.9646930950918577, 'colsample_bytree': 0.6843793208242002, 'gamma': 0.6181679306563823, 'min_child_weight': 1, 'reg_alpha': 0.08010009395246485, 'reg_lambda': 2.7899543823143893e-08}. Best is trial 36 with value: 0.9523809523809523.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejores hiperparámetros guardados en: tuning_optuna/best_params_XGBoost.json\n",
            "--- Afinamiento completo y parámetros guardados en 'tuning_optuna' ---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import optuna\n",
        "import joblib\n",
        "import json\n",
        "import warnings\n",
        "\n",
        "# Crear carpeta de resultados\\output_dir = 'tuning_optuna'\n",
        "output_dir = 'tuning_optuna'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Ignorar advertencias futuras\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# --- 1. Carga y Preparación de Datos ---\n",
        "try:\n",
        "    df_final = pd.read_csv('data_ave_B.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Advertencia: 'data_ave_B.csv' no encontrado. Usando datos de ejemplo.\")\n",
        "    data = {f'feature{i}': np.random.rand(200) for i in range(10)}\n",
        "    data['is_ave'] = np.random.randint(0, 2, 200)\n",
        "    df_final = pd.DataFrame(data)\n",
        "\n",
        "# Asegurar que la columna objetivo sea 0/1\n",
        "df_final['is_ave'] = df_final['is_ave'].astype(int)\n",
        "\n",
        "X = df_final.drop(columns=['is_ave'])\n",
        "y = df_final['is_ave']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Balanceo con SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Escalado para modelos lineales y SVM\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_bal)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Calcular ratio para XGBoost\n",
        "y_ratio = sum(y_train==0) / sum(y_train==1)\n",
        "\n",
        "# --- 2. Funciones objetivo para Optuna ---\n",
        "def objective_rf(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 300, step=50),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 50, step=5),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.5]),\n",
        "        'class_weight': trial.suggest_categorical('class_weight', ['balanced', None])\n",
        "    }\n",
        "    model = RandomForestClassifier(**params, random_state=42)\n",
        "    model.fit(X_train_bal, y_train_bal)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_proba)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "\n",
        "    trial.set_user_attr('f1', f1)\n",
        "    trial.set_user_attr('roc_auc', roc_auc)\n",
        "    trial.set_user_attr('accuracy', acc)\n",
        "    trial.set_user_attr('precision', precision)\n",
        "    trial.set_user_attr('recall', recall)\n",
        "    return f1\n",
        "\n",
        "def objective_xgb(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 300, step=50),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 50, step=5),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'scale_pos_weight': y_ratio,\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True)\n",
        "    }\n",
        "    model = XGBClassifier(**params, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "    model.fit(X_train_bal, y_train_bal)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_proba)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "\n",
        "    trial.set_user_attr('f1', f1)\n",
        "    trial.set_user_attr('roc_auc', roc_auc)\n",
        "    trial.set_user_attr('accuracy', acc)\n",
        "    trial.set_user_attr('precision', precision)\n",
        "    trial.set_user_attr('recall', recall)\n",
        "    return f1\n",
        "\n",
        "def objective_lr(trial):\n",
        "    penalty = trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet'])\n",
        "    if penalty == 'elasticnet':\n",
        "        solver = 'saga'\n",
        "    else:\n",
        "        solver = trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
        "\n",
        "    params = {\n",
        "        'C': trial.suggest_float('C', 1e-5, 100, log=True),\n",
        "        'penalty': penalty,\n",
        "        'solver': solver,\n",
        "        'l1_ratio': trial.suggest_float('l1_ratio', 0.0, 1.0) if penalty == 'elasticnet' else 0.0,\n",
        "        'class_weight': trial.suggest_categorical('class_weight', ['balanced', None])\n",
        "    }\n",
        "    model = LogisticRegression(**params, max_iter=1000, random_state=42)\n",
        "    model.fit(X_train_scaled, y_train_bal)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    y_proba = model.predict_proba(X_test_scaled)[:,1]\n",
        "\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_proba)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "\n",
        "    trial.set_user_attr('f1', f1)\n",
        "    trial.set_user_attr('roc_auc', roc_auc)\n",
        "    trial.set_user_attr('accuracy', acc)\n",
        "    trial.set_user_attr('precision', precision)\n",
        "    trial.set_user_attr('recall', recall)\n",
        "    return f1\n",
        "\n",
        "def objective_svm(trial):\n",
        "    kernel = trial.suggest_categorical('kernel', ['rbf', 'poly'])\n",
        "    params = {\n",
        "        'C': trial.suggest_float('C', 1e-3, 1000, log=True),\n",
        "        'kernel': kernel,\n",
        "        'gamma': trial.suggest_categorical('gamma', ['scale', 'auto']),\n",
        "        'class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),\n",
        "        'tol': trial.suggest_float('tol', 1e-4, 1e-2, log=True)\n",
        "    }\n",
        "    if kernel == 'poly': params['degree'] = trial.suggest_int('degree', 2, 5)\n",
        "    model = SVC(**params, probability=True, random_state=42, max_iter=10000)\n",
        "    model.fit(X_train_scaled, y_train_bal)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    y_proba = model.predict_proba(X_test_scaled)[:,1]\n",
        "\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_proba)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "\n",
        "    trial.set_user_attr('f1', f1)\n",
        "    trial.set_user_attr('roc_auc', roc_auc)\n",
        "    trial.set_user_attr('accuracy', acc)\n",
        "    trial.set_user_attr('precision', precision)\n",
        "    trial.set_user_attr('recall', recall)\n",
        "    return f1\n",
        "\n",
        "# --- 3. Bucle de Afinamiento y Guardado ---\n",
        "objectives = {'Random Forest': objective_rf, 'XGBoost': objective_xgb, 'Logistic Regression': objective_lr, 'SVM': objective_svm}\n",
        "for model_name, objective in objectives.items():\n",
        "    print(f\"--- Afinando: {model_name} ---\")\n",
        "    study = optuna.create_study(direction='maximize', storage='sqlite:///optuna_tuning.db', study_name=f\"{model_name}_tuning\", load_if_exists=True)\n",
        "    study.optimize(objective, n_trials=50)\n",
        "\n",
        "    # Guardar resultados de todos los trials\n",
        "    records = []\n",
        "    for t in study.trials:\n",
        "        rec = {'trial': t.number, 'model': model_name}\n",
        "        rec.update(t.params)\n",
        "        rec.update({k: t.user_attrs[k] for k in ['f1','roc_auc','accuracy','precision','recall']})\n",
        "        records.append(rec)\n",
        "    df_res = pd.DataFrame(records)\n",
        "    df_res.to_json(os.path.join(output_dir, f\"results_{model_name}.json\"), orient='records', indent=4)\n",
        "\n",
        "    # Guardar mejores hiperparámetros en JSON\n",
        "    best_params = study.best_params\n",
        "    summary_file = os.path.join(output_dir, f\"best_params_{model_name}.json\")\n",
        "    with open(summary_file, 'w') as f:\n",
        "        json.dump(best_params, f, indent=4)\n",
        "    print(f\"Mejores hiperparámetros guardados en: {summary_file}\")\n",
        "\n",
        "print(\"--- Afinamiento completo y parámetros guardados en 'tuning_optuna' ---\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
