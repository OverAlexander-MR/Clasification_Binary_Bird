{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR48SYPmyLQE"
      },
      "source": [
        "## Optuna para Mammalia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGKiGl8_-tX6",
        "outputId": "9042e17e-2678-4675-c7f4-818824949cc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Afinando: XGBoost ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-07-25 22:53:34,068] A new study created in RDB with name: XGBoost_tuning\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:53:34] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:54:04,438] Trial 0 finished with value: 0.8727833461835004 and parameters: {'n_estimators': 200, 'max_depth': 45, 'learning_rate': 0.013482849682105193, 'subsample': 0.9724528959991334, 'colsample_bytree': 0.8211138800402007, 'gamma': 0.05508589118781326, 'min_child_weight': 2, 'reg_alpha': 6.886542613039117e-06, 'reg_lambda': 0.08194017619767653}. Best is trial 0 with value: 0.8727833461835004.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:54:04] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:54:06,871] Trial 1 finished with value: 0.7163252638112974 and parameters: {'n_estimators': 50, 'max_depth': 20, 'learning_rate': 0.030553968096376588, 'subsample': 0.7558966026222388, 'colsample_bytree': 0.5051685513916032, 'gamma': 4.46851553721319, 'min_child_weight': 5, 'reg_alpha': 0.009857500412275347, 'reg_lambda': 0.06220841009112812}. Best is trial 0 with value: 0.8727833461835004.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:54:07] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:54:15,467] Trial 2 finished with value: 0.837037037037037 and parameters: {'n_estimators': 100, 'max_depth': 30, 'learning_rate': 0.02979029271424164, 'subsample': 0.8182297182755939, 'colsample_bytree': 0.8424389009898208, 'gamma': 2.3235861510647373, 'min_child_weight': 4, 'reg_alpha': 2.015049808388948e-08, 'reg_lambda': 1.8983660511144093e-06}. Best is trial 0 with value: 0.8727833461835004.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:54:15] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:54:19,487] Trial 3 finished with value: 0.8499234303215927 and parameters: {'n_estimators': 150, 'max_depth': 50, 'learning_rate': 0.048093752817847814, 'subsample': 0.7623727206299967, 'colsample_bytree': 0.8695850378077601, 'gamma': 3.42161781472984, 'min_child_weight': 10, 'reg_alpha': 3.420879359644928e-08, 'reg_lambda': 0.008210025818419788}. Best is trial 0 with value: 0.8727833461835004.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:54:19] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:54:21,148] Trial 4 finished with value: 0.8407148407148407 and parameters: {'n_estimators': 200, 'max_depth': 15, 'learning_rate': 0.24596358044851652, 'subsample': 0.5545540411962195, 'colsample_bytree': 0.8954817525968685, 'gamma': 2.790604984227159, 'min_child_weight': 5, 'reg_alpha': 0.0007539277050906597, 'reg_lambda': 0.0024392120472416998}. Best is trial 0 with value: 0.8727833461835004.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:54:21] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:54:27,068] Trial 5 finished with value: 0.7557301899148657 and parameters: {'n_estimators': 50, 'max_depth': 50, 'learning_rate': 0.030428728494649396, 'subsample': 0.8809508918993572, 'colsample_bytree': 0.5984101167849667, 'gamma': 0.8589117854685357, 'min_child_weight': 4, 'reg_alpha': 0.0033381540978048227, 'reg_lambda': 5.485276835245804e-05}. Best is trial 0 with value: 0.8727833461835004.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:54:27] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:54:44,659] Trial 6 finished with value: 0.8321167883211679 and parameters: {'n_estimators': 150, 'max_depth': 35, 'learning_rate': 0.013433283559793964, 'subsample': 0.7311633065922741, 'colsample_bytree': 0.7726101169228792, 'gamma': 2.5085170788343634, 'min_child_weight': 1, 'reg_alpha': 1.798252843392093e-05, 'reg_lambda': 0.009217108608406004}. Best is trial 0 with value: 0.8727833461835004.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:54:44] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:54:47,475] Trial 7 finished with value: 0.5776911836337068 and parameters: {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.012165284598280764, 'subsample': 0.5927576037259288, 'colsample_bytree': 0.7673374574375802, 'gamma': 3.60908697257772, 'min_child_weight': 10, 'reg_alpha': 0.004420025782164583, 'reg_lambda': 0.3826791339965524}. Best is trial 0 with value: 0.8727833461835004.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:54:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:54:51,381] Trial 8 finished with value: 0.7004433185560481 and parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.03466110964679664, 'subsample': 0.7596766452937125, 'colsample_bytree': 0.9555344916528468, 'gamma': 1.8270418307876923, 'min_child_weight': 10, 'reg_alpha': 1.6344089713960972e-06, 'reg_lambda': 1.6802457389917166e-05}. Best is trial 0 with value: 0.8727833461835004.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:54:51] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:55:04,567] Trial 9 finished with value: 0.8326055312954876 and parameters: {'n_estimators': 250, 'max_depth': 35, 'learning_rate': 0.012081633395478394, 'subsample': 0.9168048720773545, 'colsample_bytree': 0.5997780265581112, 'gamma': 4.055786002004959, 'min_child_weight': 5, 'reg_alpha': 2.214832327786677e-05, 'reg_lambda': 0.008032434782034845}. Best is trial 0 with value: 0.8727833461835004.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:55:04] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:55:10,284] Trial 10 finished with value: 0.8881685575364667 and parameters: {'n_estimators': 250, 'max_depth': 40, 'learning_rate': 0.11671367946869952, 'subsample': 0.9941895029916881, 'colsample_bytree': 0.6853867782187167, 'gamma': 0.3300451289239549, 'min_child_weight': 1, 'reg_alpha': 0.488378357942061, 'reg_lambda': 4.675709582289582e-08}. Best is trial 10 with value: 0.8881685575364667.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:55:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:55:18,455] Trial 11 finished with value: 0.9014084507042254 and parameters: {'n_estimators': 250, 'max_depth': 40, 'learning_rate': 0.13964593428796826, 'subsample': 0.9957519964486455, 'colsample_bytree': 0.692190707457904, 'gamma': 0.04112704231650027, 'min_child_weight': 1, 'reg_alpha': 0.12003824886556633, 'reg_lambda': 3.402368411555459e-08}. Best is trial 11 with value: 0.9014084507042254.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:55:18] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:55:23,039] Trial 12 finished with value: 0.9054726368159204 and parameters: {'n_estimators': 300, 'max_depth': 40, 'learning_rate': 0.14929402464330505, 'subsample': 0.9980166479898599, 'colsample_bytree': 0.675875921750221, 'gamma': 0.15015396361335087, 'min_child_weight': 2, 'reg_alpha': 0.9643539417016157, 'reg_lambda': 1.2705173181191421e-08}. Best is trial 12 with value: 0.9054726368159204.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:55:23] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:55:28,227] Trial 13 finished with value: 0.8728139904610492 and parameters: {'n_estimators': 300, 'max_depth': 25, 'learning_rate': 0.1296649876248684, 'subsample': 0.9055141492411313, 'colsample_bytree': 0.6801375100379402, 'gamma': 1.2622318927181801, 'min_child_weight': 3, 'reg_alpha': 0.6336965581804181, 'reg_lambda': 1.0442808346888445e-08}. Best is trial 12 with value: 0.9054726368159204.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:55:28] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:55:33,692] Trial 14 finished with value: 0.8975609756097561 and parameters: {'n_estimators': 250, 'max_depth': 40, 'learning_rate': 0.10643645496819683, 'subsample': 0.6586307343053831, 'colsample_bytree': 0.674489639923046, 'gamma': 0.8473960706381705, 'min_child_weight': 8, 'reg_alpha': 0.06717059936693728, 'reg_lambda': 2.1434828034554796e-07}. Best is trial 12 with value: 0.9054726368159204.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:55:33] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:55:35,494] Trial 15 finished with value: 0.8656957928802589 and parameters: {'n_estimators': 300, 'max_depth': 30, 'learning_rate': 0.26983395014377465, 'subsample': 0.8466593068188065, 'colsample_bytree': 0.6204463090711809, 'gamma': 1.448304721274127, 'min_child_weight': 7, 'reg_alpha': 0.06939283818161086, 'reg_lambda': 2.8157904177079117e-07}. Best is trial 12 with value: 0.9054726368159204.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:55:35] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:55:40,666] Trial 16 finished with value: 0.889253486464315 and parameters: {'n_estimators': 250, 'max_depth': 40, 'learning_rate': 0.17226961404557645, 'subsample': 0.9512889439334801, 'colsample_bytree': 0.5245857045286952, 'gamma': 0.44342367594918924, 'min_child_weight': 2, 'reg_alpha': 0.00030473437007449386, 'reg_lambda': 2.71019865887873e-06}. Best is trial 12 with value: 0.9054726368159204.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:55:40] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:55:46,365] Trial 17 finished with value: 0.8894269572235673 and parameters: {'n_estimators': 300, 'max_depth': 45, 'learning_rate': 0.07572669337286025, 'subsample': 0.9969132937073841, 'colsample_bytree': 0.7178731270306195, 'gamma': 0.8282471455685871, 'min_child_weight': 3, 'reg_alpha': 0.04275268368973504, 'reg_lambda': 1.8356564735114296e-08}. Best is trial 12 with value: 0.9054726368159204.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:55:46] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:55:53,150] Trial 18 finished with value: 0.8873015873015873 and parameters: {'n_estimators': 200, 'max_depth': 35, 'learning_rate': 0.0734095222706374, 'subsample': 0.8358530566004347, 'colsample_bytree': 0.5586094069029924, 'gamma': 1.7336320493499593, 'min_child_weight': 1, 'reg_alpha': 0.563147648683377, 'reg_lambda': 2.1119217156452984e-07}. Best is trial 12 with value: 0.9054726368159204.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:55:53] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:55:56,626] Trial 19 finished with value: 0.901571546732837 and parameters: {'n_estimators': 250, 'max_depth': 25, 'learning_rate': 0.1975782515979752, 'subsample': 0.9286910387817111, 'colsample_bytree': 0.6458841992265767, 'gamma': 0.006295085831296454, 'min_child_weight': 7, 'reg_alpha': 3.067270018341203e-07, 'reg_lambda': 0.00026063325989874805}. Best is trial 12 with value: 0.9054726368159204.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:55:56] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:55:58,819] Trial 20 finished with value: 0.8812199036918138 and parameters: {'n_estimators': 300, 'max_depth': 20, 'learning_rate': 0.21182389402540955, 'subsample': 0.9352427412111557, 'colsample_bytree': 0.6340493619576105, 'gamma': 1.2081531255246527, 'min_child_weight': 7, 'reg_alpha': 5.93145097698035e-07, 'reg_lambda': 0.0007311239602823202}. Best is trial 12 with value: 0.9054726368159204.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:55:59] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:56:04,962] Trial 21 finished with value: 0.9036544850498339 and parameters: {'n_estimators': 250, 'max_depth': 25, 'learning_rate': 0.16285735371213275, 'subsample': 0.9531543688474209, 'colsample_bytree': 0.7229962241854992, 'gamma': 0.006362632444983129, 'min_child_weight': 7, 'reg_alpha': 1.6819042503446535e-07, 'reg_lambda': 0.000329639840329645}. Best is trial 12 with value: 0.9054726368159204.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:56:05] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:56:07,662] Trial 22 finished with value: 0.8915857605177994 and parameters: {'n_estimators': 250, 'max_depth': 25, 'learning_rate': 0.18532741783738213, 'subsample': 0.8771376410308086, 'colsample_bytree': 0.7390371917533829, 'gamma': 0.48919847595426524, 'min_child_weight': 8, 'reg_alpha': 1.8281976947942732e-07, 'reg_lambda': 0.00024216716884093855}. Best is trial 12 with value: 0.9054726368159204.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:56:07] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:56:12,952] Trial 23 finished with value: 0.899513776337115 and parameters: {'n_estimators': 200, 'max_depth': 20, 'learning_rate': 0.0874079591502045, 'subsample': 0.9422237079186656, 'colsample_bytree': 0.7926923852846188, 'gamma': 0.027018959294266603, 'min_child_weight': 7, 'reg_alpha': 1.4274664809961758e-07, 'reg_lambda': 3.100366129825447e-05}. Best is trial 12 with value: 0.9054726368159204.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:56:13] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:56:16,852] Trial 24 finished with value: 0.8771358828315704 and parameters: {'n_estimators': 300, 'max_depth': 25, 'learning_rate': 0.29824812564878656, 'subsample': 0.8819217831400141, 'colsample_bytree': 0.636744999023154, 'gamma': 0.6163030839271135, 'min_child_weight': 6, 'reg_alpha': 1.572700215678738e-06, 'reg_lambda': 0.00022248822718567814}. Best is trial 12 with value: 0.9054726368159204.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:56:17] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:56:19,234] Trial 25 finished with value: 0.8742038216560509 and parameters: {'n_estimators': 250, 'max_depth': 30, 'learning_rate': 0.15719083729246544, 'subsample': 0.8115137775291095, 'colsample_bytree': 0.5620233084624178, 'gamma': 1.0783378077621573, 'min_child_weight': 8, 'reg_alpha': 0.00014953089966561557, 'reg_lambda': 1.3263553183671982e-05}. Best is trial 12 with value: 0.9054726368159204.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:56:19] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:56:21,443] Trial 26 finished with value: 0.8173147468818782 and parameters: {'n_estimators': 200, 'max_depth': 15, 'learning_rate': 0.0982332785714732, 'subsample': 0.9526210836914197, 'colsample_bytree': 0.7134992904716653, 'gamma': 4.895053203594102, 'min_child_weight': 6, 'reg_alpha': 9.919924855036553e-08, 'reg_lambda': 0.0013063059147017716}. Best is trial 12 with value: 0.9054726368159204.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:56:21] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:56:25,697] Trial 27 finished with value: 0.8704581358609794 and parameters: {'n_estimators': 300, 'max_depth': 15, 'learning_rate': 0.057704098955969584, 'subsample': 0.9071208366491433, 'colsample_bytree': 0.6540025155884802, 'gamma': 1.688815466649712, 'min_child_weight': 9, 'reg_alpha': 3.982668476046463e-06, 'reg_lambda': 4.398951053384111e-06}. Best is trial 12 with value: 0.9054726368159204.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:56:26] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:56:30,176] Trial 28 finished with value: 0.8874493927125506 and parameters: {'n_estimators': 150, 'max_depth': 25, 'learning_rate': 0.20516942470518734, 'subsample': 0.7064958985726264, 'colsample_bytree': 0.7373862942663342, 'gamma': 0.33873906143122134, 'min_child_weight': 6, 'reg_alpha': 4.130330602971079e-05, 'reg_lambda': 0.0001289591245436216}. Best is trial 12 with value: 0.9054726368159204.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:56:30] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:56:34,034] Trial 29 finished with value: 0.913151364764268 and parameters: {'n_estimators': 200, 'max_depth': 45, 'learning_rate': 0.13844874508172178, 'subsample': 0.9718232725684344, 'colsample_bytree': 0.8070099766726937, 'gamma': 0.08437157606656463, 'min_child_weight': 9, 'reg_alpha': 1.0220713597350852e-08, 'reg_lambda': 0.04867175877489809}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:56:34] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:56:41,546] Trial 30 finished with value: 0.8904991948470209 and parameters: {'n_estimators': 200, 'max_depth': 45, 'learning_rate': 0.06367283817602525, 'subsample': 0.9773873767668345, 'colsample_bytree': 0.8206152732201285, 'gamma': 0.6709188963258034, 'min_child_weight': 9, 'reg_alpha': 1.1595921626034744e-08, 'reg_lambda': 0.6093267874836925}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:56:41] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:56:45,629] Trial 31 finished with value: 0.9041769041769042 and parameters: {'n_estimators': 250, 'max_depth': 50, 'learning_rate': 0.1517126455141257, 'subsample': 0.9470629449043434, 'colsample_bytree': 0.8049944222589052, 'gamma': 0.006655230531328581, 'min_child_weight': 9, 'reg_alpha': 5.783990648983705e-08, 'reg_lambda': 0.05910338327843863}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:56:45] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:56:49,259] Trial 32 finished with value: 0.9061224489795918 and parameters: {'n_estimators': 200, 'max_depth': 50, 'learning_rate': 0.1419032391962395, 'subsample': 0.9525706789095402, 'colsample_bytree': 0.8069913040543807, 'gamma': 0.2983233148129559, 'min_child_weight': 9, 'reg_alpha': 3.7009224390029664e-08, 'reg_lambda': 0.05739071806221706}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:56:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:56:54,852] Trial 33 finished with value: 0.9018003273322422 and parameters: {'n_estimators': 200, 'max_depth': 45, 'learning_rate': 0.13017697055868235, 'subsample': 0.9649675260300766, 'colsample_bytree': 0.8155049272893404, 'gamma': 0.36995273882012264, 'min_child_weight': 9, 'reg_alpha': 4.482600056903159e-08, 'reg_lambda': 0.07667412517037951}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:56:55] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:57:02,783] Trial 34 finished with value: 0.8185104844540854 and parameters: {'n_estimators': 150, 'max_depth': 50, 'learning_rate': 0.020769201190015425, 'subsample': 0.8629345540347513, 'colsample_bytree': 0.8925312610992702, 'gamma': 0.32903103295017916, 'min_child_weight': 9, 'reg_alpha': 2.6542169892201407e-08, 'reg_lambda': 0.17935890982546812}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:57:03] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:57:09,837] Trial 35 finished with value: 0.8667711598746082 and parameters: {'n_estimators': 200, 'max_depth': 50, 'learning_rate': 0.04607614000191781, 'subsample': 0.8007128452849289, 'colsample_bytree': 0.8572103070996281, 'gamma': 2.0992642928949174, 'min_child_weight': 10, 'reg_alpha': 4.8770834005273e-08, 'reg_lambda': 0.02509199584568888}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:57:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:57:13,824] Trial 36 finished with value: 0.8837209302325582 and parameters: {'n_estimators': 150, 'max_depth': 45, 'learning_rate': 0.09366336168743679, 'subsample': 0.9737047085825743, 'colsample_bytree': 0.9393952360319644, 'gamma': 0.977310729846689, 'min_child_weight': 8, 'reg_alpha': 7.48281728845183e-07, 'reg_lambda': 0.025936987741881547}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:57:14] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:57:17,473] Trial 37 finished with value: 0.8334624322230829 and parameters: {'n_estimators': 200, 'max_depth': 50, 'learning_rate': 0.2344060418709241, 'subsample': 0.8941611580198489, 'colsample_bytree': 0.800874686753417, 'gamma': 3.0886609899440955, 'min_child_weight': 4, 'reg_alpha': 1.0137510433788482e-08, 'reg_lambda': 0.23699406401923637}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:57:17] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:57:20,349] Trial 38 finished with value: 0.8817377312952535 and parameters: {'n_estimators': 100, 'max_depth': 45, 'learning_rate': 0.14613818538622247, 'subsample': 0.922348383465385, 'colsample_bytree': 0.9140084410897525, 'gamma': 0.6674991205704662, 'min_child_weight': 9, 'reg_alpha': 0.0010631490182434159, 'reg_lambda': 0.0027667382546989836}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:57:20] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:57:22,733] Trial 39 finished with value: 0.8679245283018868 and parameters: {'n_estimators': 100, 'max_depth': 50, 'learning_rate': 0.11567325950090837, 'subsample': 0.5372254991422147, 'colsample_bytree': 0.8443362228161632, 'gamma': 1.4144821175535152, 'min_child_weight': 10, 'reg_alpha': 5.9528468991057406e-08, 'reg_lambda': 0.8449357193979123}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:57:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:57:26,750] Trial 40 finished with value: 0.8846459824980112 and parameters: {'n_estimators': 150, 'max_depth': 45, 'learning_rate': 0.07606706774681993, 'subsample': 0.6347663488744743, 'colsample_bytree': 0.7807973801278228, 'gamma': 0.32136819081932666, 'min_child_weight': 8, 'reg_alpha': 6.309601353698132e-06, 'reg_lambda': 0.02659032903355246}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:57:27] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:57:33,846] Trial 41 finished with value: 0.9023789991796555 and parameters: {'n_estimators': 250, 'max_depth': 50, 'learning_rate': 0.16714815719828555, 'subsample': 0.9794647737533678, 'colsample_bytree': 0.7597985203941272, 'gamma': 0.14427245622194254, 'min_child_weight': 9, 'reg_alpha': 1.7967684449431795e-08, 'reg_lambda': 0.0057271719070865115}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:57:34] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:57:36,837] Trial 42 finished with value: 0.895645028759244 and parameters: {'n_estimators': 250, 'max_depth': 40, 'learning_rate': 0.24144566684739924, 'subsample': 0.9552287310015674, 'colsample_bytree': 0.828841329643205, 'gamma': 0.2043982382205548, 'min_child_weight': 5, 'reg_alpha': 2.9936769733174863e-07, 'reg_lambda': 0.1634716547233419}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:57:37] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:57:39,667] Trial 43 finished with value: 0.8822095857026807 and parameters: {'n_estimators': 200, 'max_depth': 35, 'learning_rate': 0.1537296332368598, 'subsample': 0.9988854081893279, 'colsample_bytree': 0.8776150963372215, 'gamma': 0.62243318406801, 'min_child_weight': 10, 'reg_alpha': 0.012317622233164746, 'reg_lambda': 0.055788756353818185}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:57:39] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:57:45,867] Trial 44 finished with value: 0.9078838174273859 and parameters: {'n_estimators': 250, 'max_depth': 50, 'learning_rate': 0.12091731229445155, 'subsample': 0.7886250261506311, 'colsample_bytree': 0.7175195361767215, 'gamma': 0.03506067464732335, 'min_child_weight': 8, 'reg_alpha': 8.767399758255522e-08, 'reg_lambda': 0.013799712235839949}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:57:46] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:57:49,512] Trial 45 finished with value: 0.893030794165316 and parameters: {'n_estimators': 300, 'max_depth': 50, 'learning_rate': 0.11483517405887868, 'subsample': 0.7860840433073679, 'colsample_bytree': 0.761890272670109, 'gamma': 0.812297357898804, 'min_child_weight': 9, 'reg_alpha': 3.316965344470397e-08, 'reg_lambda': 0.012945603636784719}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:57:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:57:51,763] Trial 46 finished with value: 0.8640625 and parameters: {'n_estimators': 250, 'max_depth': 45, 'learning_rate': 0.1338179149577992, 'subsample': 0.7099707526731912, 'colsample_bytree': 0.7983656387534611, 'gamma': 2.5572916081825925, 'min_child_weight': 10, 'reg_alpha': 1.41942138287077e-06, 'reg_lambda': 0.0031196506965857243}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:57:52] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:58:05,265] Trial 47 finished with value: 0.8671328671328671 and parameters: {'n_estimators': 300, 'max_depth': 45, 'learning_rate': 0.01890627669791373, 'subsample': 0.8319311278639063, 'colsample_bytree': 0.7051200326811773, 'gamma': 0.2007989110539351, 'min_child_weight': 8, 'reg_alpha': 8.90724355936315e-08, 'reg_lambda': 0.04805612422045972}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:58:05] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:58:11,859] Trial 48 finished with value: 0.8475609756097561 and parameters: {'n_estimators': 250, 'max_depth': 50, 'learning_rate': 0.04318055075531352, 'subsample': 0.5819180228888056, 'colsample_bytree': 0.6683973638195037, 'gamma': 3.858879561211881, 'min_child_weight': 3, 'reg_alpha': 4.643069714517816e-07, 'reg_lambda': 0.01410099336149217}. Best is trial 29 with value: 0.913151364764268.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [22:58:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "[I 2025-07-25 22:58:16,051] Trial 49 finished with value: 0.8764940239043825 and parameters: {'n_estimators': 50, 'max_depth': 40, 'learning_rate': 0.1049855019543645, 'subsample': 0.8589353424465055, 'colsample_bytree': 0.7494641773095196, 'gamma': 0.49724330624019053, 'min_child_weight': 2, 'reg_alpha': 2.06220481710385e-08, 'reg_lambda': 0.10832555054710047}. Best is trial 29 with value: 0.913151364764268.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejores hiperparámetros guardados en: tuning_optuna/best_params_XGBoost.json\n",
            "--- Afinamiento completo y parámetros guardados en 'tuning_optuna' ---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import optuna\n",
        "import joblib\n",
        "import json\n",
        "import warnings\n",
        "\n",
        "# Crear carpeta de resultados\\output_dir = 'tuning_optuna'\n",
        "output_dir = 'tuning_optuna'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Ignorar advertencias futuras\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# --- 1. Carga y Preparación de Datos ---\n",
        "try:\n",
        "    df_final = pd.read_csv('data_mammalia_B.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Advertencia: 'data_mammalia_B.csv' no encontrado. Usando datos de ejemplo.\")\n",
        "    data = {f'feature{i}': np.random.rand(200) for i in range(10)}\n",
        "    data['is_mammalia'] = np.random.randint(0, 2, 200)\n",
        "    df_final = pd.DataFrame(data)\n",
        "\n",
        "# Asegurar que la columna objetivo sea 0/1\n",
        "df_final['is_mammalia'] = df_final['is_mammalia'].astype(int)\n",
        "\n",
        "X = df_final.drop(columns=['is_mammalia'])\n",
        "y = df_final['is_mammalia']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Balanceo con SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Escalado para modelos lineales y SVM\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_bal)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Calcular ratio para XGBoost\n",
        "y_ratio = sum(y_train==0) / sum(y_train==1)\n",
        "\n",
        "# --- 2. Funciones objetivo para Optuna ---\n",
        "def objective_rf(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 300, step=50),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 50, step=5),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.5]),\n",
        "        'class_weight': trial.suggest_categorical('class_weight', ['balanced', None])\n",
        "    }\n",
        "    model = RandomForestClassifier(**params, random_state=42)\n",
        "    model.fit(X_train_bal, y_train_bal)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_proba)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "\n",
        "    trial.set_user_attr('f1', f1)\n",
        "    trial.set_user_attr('roc_auc', roc_auc)\n",
        "    trial.set_user_attr('accuracy', acc)\n",
        "    trial.set_user_attr('precision', precision)\n",
        "    trial.set_user_attr('recall', recall)\n",
        "    return f1\n",
        "\n",
        "def objective_xgb(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 300, step=50),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 50, step=5),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'scale_pos_weight': y_ratio,\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True)\n",
        "    }\n",
        "    model = XGBClassifier(**params, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "    model.fit(X_train_bal, y_train_bal)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_proba)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "\n",
        "    trial.set_user_attr('f1', f1)\n",
        "    trial.set_user_attr('roc_auc', roc_auc)\n",
        "    trial.set_user_attr('accuracy', acc)\n",
        "    trial.set_user_attr('precision', precision)\n",
        "    trial.set_user_attr('recall', recall)\n",
        "    return f1\n",
        "\n",
        "def objective_lr(trial):\n",
        "    penalty = trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet'])\n",
        "    if penalty == 'elasticnet':\n",
        "        solver = 'saga'\n",
        "    else:\n",
        "        solver = trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
        "\n",
        "    params = {\n",
        "        'C': trial.suggest_float('C', 1e-5, 100, log=True),\n",
        "        'penalty': penalty,\n",
        "        'solver': solver,\n",
        "        'l1_ratio': trial.suggest_float('l1_ratio', 0.0, 1.0) if penalty == 'elasticnet' else 0.0,\n",
        "        'class_weight': trial.suggest_categorical('class_weight', ['balanced', None])\n",
        "    }\n",
        "    model = LogisticRegression(**params, max_iter=1000, random_state=42)\n",
        "    model.fit(X_train_scaled, y_train_bal)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    y_proba = model.predict_proba(X_test_scaled)[:,1]\n",
        "\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_proba)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "\n",
        "    trial.set_user_attr('f1', f1)\n",
        "    trial.set_user_attr('roc_auc', roc_auc)\n",
        "    trial.set_user_attr('accuracy', acc)\n",
        "    trial.set_user_attr('precision', precision)\n",
        "    trial.set_user_attr('recall', recall)\n",
        "    return f1\n",
        "\n",
        "def objective_svm(trial):\n",
        "    kernel = trial.suggest_categorical('kernel', ['rbf', 'poly'])\n",
        "    params = {\n",
        "        'C': trial.suggest_float('C', 1e-3, 1000, log=True),\n",
        "        'kernel': kernel,\n",
        "        'gamma': trial.suggest_categorical('gamma', ['scale', 'auto']),\n",
        "        'class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),\n",
        "        'tol': trial.suggest_float('tol', 1e-4, 1e-2, log=True)\n",
        "    }\n",
        "    if kernel == 'poly': params['degree'] = trial.suggest_int('degree', 2, 5)\n",
        "    model = SVC(**params, probability=True, random_state=42, max_iter=10000)\n",
        "    model.fit(X_train_scaled, y_train_bal)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    y_proba = model.predict_proba(X_test_scaled)[:,1]\n",
        "\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_proba)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "\n",
        "    trial.set_user_attr('f1', f1)\n",
        "    trial.set_user_attr('roc_auc', roc_auc)\n",
        "    trial.set_user_attr('accuracy', acc)\n",
        "    trial.set_user_attr('precision', precision)\n",
        "    trial.set_user_attr('recall', recall)\n",
        "    return f1\n",
        "\n",
        "# --- 3. Bucle de Afinamiento y Guardado ---\n",
        "objectives = {'Random Forest': objective_rf, 'XGBoost': objective_xgb, 'Logistic Regression': objective_lr, 'SVM': objective_svm}\n",
        "for model_name, objective in objectives.items():\n",
        "    print(f\"--- Afinando: {model_name} ---\")\n",
        "    study = optuna.create_study(direction='maximize', storage='sqlite:///optuna_tuning.db', study_name=f\"{model_name}_tuning\", load_if_exists=True)\n",
        "    study.optimize(objective, n_trials=50)\n",
        "\n",
        "    # Guardar resultados de todos los trials\n",
        "    records = []\n",
        "    for t in study.trials:\n",
        "        rec = {'trial': t.number, 'model': model_name}\n",
        "        rec.update(t.params)\n",
        "        rec.update({k: t.user_attrs[k] for k in ['f1','roc_auc','accuracy','precision','recall']})\n",
        "        records.append(rec)\n",
        "    df_res = pd.DataFrame(records)\n",
        "    df_res.to_json(os.path.join(output_dir, f\"results_{model_name}.json\"), orient='records', indent=4)\n",
        "\n",
        "    # Guardar mejores hiperparámetros en JSON\n",
        "    best_params = study.best_params\n",
        "    summary_file = os.path.join(output_dir, f\"best_params_{model_name}.json\")\n",
        "    with open(summary_file, 'w') as f:\n",
        "        json.dump(best_params, f, indent=4)\n",
        "    print(f\"Mejores hiperparámetros guardados en: {summary_file}\")\n",
        "\n",
        "print(\"--- Afinamiento completo y parámetros guardados en 'tuning_optuna' ---\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
